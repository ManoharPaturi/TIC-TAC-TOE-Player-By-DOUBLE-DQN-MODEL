# -*- coding: utf-8 -*-
"""Advanced Tic-Tac-Toe AI Agent (Double Dueling DQN)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18tKMKPYv7QSpy9wpv4_NrhPF_34u6ydM
"""

import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import namedtuple, deque
import matplotlib.pyplot as plt
import math

# --- Game Environment ---
class TicTacToe:
    """A class to represent the Tic-Tac-Toe game environment."""
    def __init__(self):
        self.board = np.zeros(9, dtype=np.float32)
        self.current_winner = None

    def available_moves(self):
        return [i for i, spot in enumerate(self.board) if spot == 0]

    def make_move(self, square, letter):
        if self.board[square] == 0:
            self.board[square] = letter
            if self.winner(square, letter):
                self.current_winner = letter
            return True
        return False

    def winner(self, square, letter):
        # Check row, column, and diagonals for a win
        row_ind = square // 3
        if all(self.board[row_ind*3 + i] == letter for i in range(3)): return True
        col_ind = square % 3
        if all(self.board[col_ind + i*3] == letter for i in range(3)): return True
        if square % 2 == 0:
            if all(self.board[i] == letter for i in [0, 4, 8]): return True
            if all(self.board[i] == letter for i in [2, 4, 6]): return True
        return False

    def get_state(self):
        return self.board.copy()

    def reset(self):
        self.board = np.zeros(9, dtype=np.float32)
        self.current_winner = None
        return self.get_state()

# --- Noisy Layer for Exploration ---
class NoisyLinear(nn.Module):
    """
    NoisyNet layer for exploration. Replaces a standard nn.Linear layer.
    This layer adds learnable noise to its weights, encouraging exploration.
    """
    def __init__(self, in_features, out_features, std_init=0.5):
        super(NoisyLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))

        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))
        self.register_buffer('bias_epsilon', torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()

    def reset_parameters(self):
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)
        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size):
        return torch.randn(size).sign() * torch.randn(size).abs().sqrt()

    def forward(self, x):
        if self.training:
            return F.linear(x, self.weight_mu + self.weight_sigma * self.weight_epsilon,
                            self.bias_mu + self.bias_sigma * self.bias_epsilon)
        else:
            return F.linear(x, self.weight_mu, self.bias_mu)

# --- Dueling DQN with Noisy Layers ---
class DuelingDQN(nn.Module):
    """A Dueling DQN architecture using NoisyLinear layers for exploration."""
    def __init__(self):
        super(DuelingDQN, self).__init__()
        self.feature_layer = nn.Sequential(
            nn.Linear(9, 128),
            nn.ReLU()
        )
        self.value_stream = nn.Sequential(
            NoisyLinear(128, 128),
            nn.ReLU(),
            NoisyLinear(128, 1)
        )
        self.advantage_stream = nn.Sequential(
            NoisyLinear(128, 128),
            nn.ReLU(),
            NoisyLinear(128, 9)
        )

    def forward(self, x):
        features = self.feature_layer(x)
        values = self.value_stream(features)
        advantages = self.advantage_stream(features)
        qvals = values + (advantages - advantages.mean(dim=1, keepdim=True))
        return qvals

    def reset_noise(self):
        for name, module in self.named_children():
            if 'stream' in name:
                for layer in module:
                    if isinstance(layer, NoisyLinear):
                        layer.reset_noise()

# --- Prioritized Experience Replay (PER) ---
class ReplayMemory:
    """
    A prioritized replay memory that stores transitions and their priorities.
    Experiences with higher TD-error (more "surprising") are more likely to be sampled.
    """
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.alpha = alpha
        self.memory = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
        self.max_priority = 1.0

    def push(self, state, action, next_state, reward):
        if len(self.memory) < self.capacity:
            self.memory.append(None)

        self.memory[self.position] = (state, action, next_state, reward)
        self.priorities[self.position] = self.max_priority
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size, beta=0.4):
        if len(self.memory) == 0:
            return [], [], []

        if len(self.memory) == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.position]

        probs = prios ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.memory), batch_size, p=probs)
        samples = [self.memory[i] for i in indices]

        total = len(self.memory)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        weights = np.array(weights, dtype=np.float32)

        return samples, indices, weights

    def update_priorities(self, batch_indices, batch_priorities):
        for idx, prio in zip(batch_indices, batch_priorities):
            self.priorities[idx] = prio
        self.max_priority = max(self.max_priority, np.max(batch_priorities))

    def __len__(self):
        return len(self.memory)

# --- Agent ---
class DQNAgent:
    def __init__(self, player_letter, batch_size=128, gamma=0.99, tau=0.005, lr=1e-5): # *** LEARNING RATE ADJUSTED ***
        self.player_letter = player_letter
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.lr = lr

        self.policy_net = DuelingDQN().to(self.device)
        self.target_net = DuelingDQN().to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.lr, amsgrad=True)
        self.memory = ReplayMemory(10000)

    def select_action(self, state, available_moves):
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
            q_values = self.policy_net(state_tensor)
            mask = torch.full(q_values.shape, -float('inf'), device=self.device)
            mask[0, available_moves] = 0
            q_values += mask
            return q_values.max(1)[1].view(1, 1)

    def optimize_model(self, beta):
        if len(self.memory) < self.batch_size:
            return None

        transitions, indices, weights = self.memory.sample(self.batch_size, beta)
        if not transitions:
            return None

        batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)

        # Convert to tensors
        state_batch = torch.tensor(np.array(batch_state), device=self.device, dtype=torch.float32)
        action_batch = torch.cat([a for a in batch_action])
        reward_batch = torch.tensor(np.array(batch_reward), device=self.device, dtype=torch.float32)
        weights_batch = torch.tensor(weights, device=self.device, dtype=torch.float32)

        non_final_mask = torch.tensor([s is not None for s in batch_next_state], device=self.device, dtype=torch.bool)
        non_final_next_states_list = [s for s in batch_next_state if s is not None]
        if not non_final_next_states_list: # Handle case where all next_states are None
            non_final_next_states = torch.empty(0, 9, device=self.device, dtype=torch.float32)
        else:
            non_final_next_states = torch.tensor(np.array(non_final_next_states_list), device=self.device, dtype=torch.float32)

        # Get current Q values
        state_action_values = self.policy_net(state_batch).gather(1, action_batch)

        # Get next state Q values (Double DQN)
        next_state_values = torch.zeros(self.batch_size, device=self.device)
        if len(non_final_next_states_list) > 0:
            with torch.no_grad():
                policy_next_actions = self.policy_net(non_final_next_states).max(1)[1].unsqueeze(1)
                next_state_values[non_final_mask] = self.target_net(non_final_next_states).gather(1, policy_next_actions).squeeze()

        expected_state_action_values = (next_state_values * self.gamma) + reward_batch

        # Compute loss (and update priorities for PER)
        td_error = (expected_state_action_values.unsqueeze(1) - state_action_values).abs()
        new_priorities = td_error.squeeze().detach().cpu().numpy() + 1e-5
        self.memory.update_priorities(indices, new_priorities)

        criterion = nn.SmoothL1Loss(reduction='none')
        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
        loss = (loss.squeeze() * weights_batch).mean()

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 10)
        self.optimizer.step()

        # Reset noise for exploration
        self.policy_net.reset_noise()
        self.target_net.reset_noise()

        return loss.item()

    def update_target_net(self):
        target_net_state_dict = self.target_net.state_dict()
        policy_net_state_dict = self.policy_net.state_dict()
        for key in policy_net_state_dict:
            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + policy_net_state_dict[key]*(1-self.tau)
        self.target_net.load_state_dict(target_net_state_dict)

# --- Training Loop ---
def train(num_episodes=25000):
    game = TicTacToe()
    agent1 = DQNAgent(player_letter=1)
    agent2 = DQNAgent(player_letter=-1)
    agents = {1: agent1, -1: agent2}

    history = {'loss': [], 'wins_x': [], 'wins_o': [], 'draws': []}
    win_counts = {1: 0, -1: 0, 'draw': 0}

    beta_start = 0.4
    beta_frames = num_episodes
    beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)

    for i_episode in range(num_episodes):
        state = game.reset()
        current_player = 1
        done = False
        episode_transitions = []

        while not done:
            agent = agents[current_player]
            available_moves = game.available_moves()

            if not available_moves:
                done = True
                win_counts['draw'] += 1
                break

            action = agent.select_action(state, available_moves)

            # Store the state and action
            episode_transitions.append({'state': state, 'action': action, 'player': current_player})

            game.make_move(action.item(), current_player)
            next_state = game.get_state()

            if game.current_winner is not None:
                done = True
                win_counts[game.current_winner] += 1

            state = next_state
            if not done:
                current_player = -current_player

        # *** CORRECTED REWARD ASSIGNMENT ***
        # After the episode is over, assign rewards and push to memory
        final_reward = {1: 0, -1: 0}
        if game.current_winner is not None:
            final_reward[game.current_winner] = 1
            final_reward[-game.current_winner] = -1

        for i, transition in enumerate(episode_transitions):
            player = transition['player']
            s = transition['state']
            a = transition['action']

            # Determine the next state for this transition
            if i == len(episode_transitions) - 1:
                ns = None # Terminal state
            else:
                ns = episode_transitions[i+1]['state']

            # Assign reward
            if done and i == len(episode_transitions) - 1:
                r = final_reward[player]
            else:
                r = 0 # Intermediate moves have 0 reward

            agents[player].memory.push(s, a, ns, r)

        # Perform optimization for both agents
        beta = beta_by_frame(i_episode)
        loss1 = agents[1].optimize_model(beta)
        loss2 = agents[-1].optimize_model(beta)

        agents[1].update_target_net()
        agents[-1].update_target_net()

        avg_loss = np.mean([l for l in [loss1, loss2] if l is not None])
        if not np.isnan(avg_loss):
            history['loss'].append(avg_loss)

        if (i_episode + 1) % 100 == 0:
            history['wins_x'].append(win_counts[1])
            history['wins_o'].append(win_counts[-1])
            history['draws'].append(win_counts['draw'])
            win_counts = {1: 0, -1: 0, 'draw': 0} # Reset
            print(f"Episode {i_episode+1}/{num_episodes}, Avg Loss: {np.mean(history['loss'][-100:]):.4f}")

    # --- Plotting Results ---
    fig, axs = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('State-of-the-Art Agent Training Performance', fontsize=16)

    # Loss Plot
    axs[0, 0].plot(history['loss'], label='Loss')
    loss_ma = np.convolve(history['loss'], np.ones(100)/100, mode='valid')
    axs[0, 0].plot(loss_ma, label='Moving Average (100 ep)')
    axs[0, 0].set_title('Training Loss per Episode')
    axs[0, 0].set_xlabel('Episode')
    axs[0, 0].set_ylabel('Loss')
    axs[0, 0].legend()
    axs[0, 0].grid(True)

    # Win Rate Plot
    total_games_chunk = np.array(history['wins_x']) + np.array(history['wins_o']) + np.array(history['draws'])
    # Avoid division by zero if a chunk has no games
    total_games_chunk[total_games_chunk == 0] = 1
    win_rate_x = np.array(history['wins_x']) / total_games_chunk
    win_rate_o = np.array(history['wins_o']) / total_games_chunk

    axs[0, 1].plot(win_rate_x, label='X Win Rate', color='blue')
    axs[0, 1].plot(win_rate_o, label='O Win Rate', color='red')
    axs[0, 1].set_title('Win Rate per 100 Episodes')
    axs[0, 1].set_xlabel('Episode Chunk (x100)')
    axs[0, 1].set_ylabel('Rate')
    axs[0, 1].legend()
    axs[0, 1].grid(True)

    # Draw Rate Plot
    draw_rate = np.array(history['draws']) / total_games_chunk
    axs[1, 0].plot(draw_rate, label='Draw Rate', color='green')
    axs[1, 0].set_title('Draw Rate per 100 Episodes')
    axs[1, 0].set_xlabel('Episode Chunk (x100)')
    axs[1, 0].set_ylabel('Rate')
    axs[1, 0].legend()
    axs[1, 0].grid(True)

    # Cumulative Wins Plot
    cum_wins_x = np.cumsum(history['wins_x'])
    cum_wins_o = np.cumsum(history['wins_o'])
    cum_draws = np.cumsum(history['draws'])
    axs[1, 1].stackplot(range(len(cum_wins_x)), cum_wins_x, cum_wins_o, cum_draws,
                        labels=['X Wins', 'O Wins', 'Draws'],
                        colors=['blue', 'red', 'green'])
    axs[1, 1].set_title('Cumulative Game Outcomes')
    axs[1, 1].set_xlabel('Episode Chunk (x100)')
    axs[1, 1].set_ylabel('Total Games')
    axs[1, 1].legend(loc='upper left')
    axs[1, 1].grid(True)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

if __name__ == '__main__':
    train()

import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import namedtuple, deque
import matplotlib.pyplot as plt
import math

# --- Game Environment ---
class TicTacToe:
    """A class to represent the Tic-Tac-Toe game environment."""
    def __init__(self):
        self.board = np.zeros(9, dtype=np.float32)
        self.current_winner = None

    def available_moves(self):
        return [i for i, spot in enumerate(self.board) if spot == 0]

    def make_move(self, square, letter):
        if self.board[square] == 0:
            self.board[square] = letter
            if self.winner(square, letter):
                self.current_winner = letter
            return True
        return False

    def winner(self, square, letter):
        # Check row, column, and diagonals for a win
        row_ind = square // 3
        if all(self.board[row_ind*3 + i] == letter for i in range(3)): return True
        col_ind = square % 3
        if all(self.board[col_ind + i*3] == letter for i in range(3)): return True
        if square % 2 == 0:
            if all(self.board[i] == letter for i in [0, 4, 8]): return True
            if all(self.board[i] == letter for i in [2, 4, 6]): return True
        return False

    def get_state(self):
        return self.board.copy()

    def reset(self):
        self.board = np.zeros(9, dtype=np.float32)
        self.current_winner = None
        return self.get_state()

# --- Noisy Layer for Exploration ---
class NoisyLinear(nn.Module):
    """
    NoisyNet layer for exploration. Replaces a standard nn.Linear layer.
    This layer adds learnable noise to its weights, encouraging exploration.
    """
    def __init__(self, in_features, out_features, std_init=0.5):
        super(NoisyLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))
        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))
        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))

        self.bias_mu = nn.Parameter(torch.empty(out_features))
        self.bias_sigma = nn.Parameter(torch.empty(out_features))
        self.register_buffer('bias_epsilon', torch.empty(out_features))

        self.reset_parameters()
        self.reset_noise()

    def reset_parameters(self):
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))

    def reset_noise(self):
        epsilon_in = self._scale_noise(self.in_features)
        epsilon_out = self._scale_noise(self.out_features)
        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def _scale_noise(self, size):
        return torch.randn(size).sign() * torch.randn(size).abs().sqrt()

    def forward(self, x):
        if self.training:
            return F.linear(x, self.weight_mu + self.weight_sigma * self.weight_epsilon,
                            self.bias_mu + self.bias_sigma * self.bias_epsilon)
        else:
            return F.linear(x, self.weight_mu, self.bias_mu)

# --- Dueling DQN with Noisy Layers ---
class DuelingDQN(nn.Module):
    """A Dueling DQN architecture using NoisyLinear layers for exploration."""
    def __init__(self):
        super(DuelingDQN, self).__init__()
        self.feature_layer = nn.Sequential(
            nn.Linear(9, 128),
            nn.ReLU()
        )
        self.value_stream = nn.Sequential(
            NoisyLinear(128, 128),
            nn.ReLU(),
            NoisyLinear(128, 1)
        )
        self.advantage_stream = nn.Sequential(
            NoisyLinear(128, 128),
            nn.ReLU(),
            NoisyLinear(128, 9)
        )

    def forward(self, x):
        features = self.feature_layer(x)
        values = self.value_stream(features)
        advantages = self.advantage_stream(features)
        qvals = values + (advantages - advantages.mean(dim=1, keepdim=True))
        return qvals

    def reset_noise(self):
        for name, module in self.named_children():
            if 'stream' in name:
                for layer in module:
                    if isinstance(layer, NoisyLinear):
                        layer.reset_noise()

# --- Prioritized Experience Replay (PER) ---
class ReplayMemory:
    """
    A prioritized replay memory that stores transitions and their priorities.
    Experiences with higher TD-error (more "surprising") are more likely to be sampled.
    """
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.alpha = alpha
        self.memory = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
        self.max_priority = 1.0

    def push(self, state, action, next_state, reward):
        if len(self.memory) < self.capacity:
            self.memory.append(None)

        self.memory[self.position] = (state, action, next_state, reward)
        self.priorities[self.position] = self.max_priority
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size, beta=0.4):
        if len(self.memory) == 0:
            return [], [], []

        if len(self.memory) == self.capacity:
            prios = self.priorities
        else:
            prios = self.priorities[:self.position]

        probs = prios ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.memory), batch_size, p=probs)
        samples = [self.memory[i] for i in indices]

        total = len(self.memory)
        weights = (total * probs[indices]) ** (-beta)
        weights /= weights.max()
        weights = np.array(weights, dtype=np.float32)

        return samples, indices, weights

    def update_priorities(self, batch_indices, batch_priorities):
        for idx, prio in zip(batch_indices, batch_priorities):
            self.priorities[idx] = prio
        self.max_priority = max(self.max_priority, np.max(batch_priorities))

    def __len__(self):
        return len(self.memory)

# --- Agent ---
class DQNAgent:
    def __init__(self, player_letter, batch_size=128, gamma=0.99, tau=0.005, lr=1e-5): # *** LEARNING RATE ADJUSTED ***
        self.player_letter = player_letter
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = batch_size
        self.gamma = gamma
        self.tau = tau
        self.lr = lr

        self.policy_net = DuelingDQN().to(self.device)
        self.target_net = DuelingDQN().to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.lr, amsgrad=True)
        self.memory = ReplayMemory(10000)

    def select_action(self, state, available_moves):
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
            q_values = self.policy_net(state_tensor)
            mask = torch.full(q_values.shape, -float('inf'), device=self.device)
            mask[0, available_moves] = 0
            q_values += mask
            return q_values.max(1)[1].view(1, 1)

    def optimize_model(self, beta):
        if len(self.memory) < self.batch_size:
            return None

        transitions, indices, weights = self.memory.sample(self.batch_size, beta)
        if not transitions:
            return None

        batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)

        # Convert to tensors
        state_batch = torch.tensor(np.array(batch_state), device=self.device, dtype=torch.float32)
        action_batch = torch.cat([a for a in batch_action])
        reward_batch = torch.tensor(np.array(batch_reward), device=self.device, dtype=torch.float32)
        weights_batch = torch.tensor(weights, device=self.device, dtype=torch.float32)

        non_final_mask = torch.tensor([s is not None for s in batch_next_state], device=self.device, dtype=torch.bool)
        non_final_next_states_list = [s for s in batch_next_state if s is not None]
        if not non_final_next_states_list: # Handle case where all next_states are None
            non_final_next_states = torch.empty(0, 9, device=self.device, dtype=torch.float32)
        else:
            non_final_next_states = torch.tensor(np.array(non_final_next_states_list), device=self.device, dtype=torch.float32)

        # Get current Q values
        state_action_values = self.policy_net(state_batch).gather(1, action_batch)

        # Get next state Q values (Double DQN)
        next_state_values = torch.zeros(self.batch_size, device=self.device)
        if len(non_final_next_states_list) > 0:
            with torch.no_grad():
                policy_next_actions = self.policy_net(non_final_next_states).max(1)[1].unsqueeze(1)
                next_state_values[non_final_mask] = self.target_net(non_final_next_states).gather(1, policy_next_actions).squeeze()

        expected_state_action_values = (next_state_values * self.gamma) + reward_batch

        # Compute loss (and update priorities for PER)
        td_error = (expected_state_action_values.unsqueeze(1) - state_action_values).abs()
        new_priorities = td_error.squeeze().detach().cpu().numpy() + 1e-5
        self.memory.update_priorities(indices, new_priorities)

        criterion = nn.SmoothL1Loss(reduction='none')
        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))
        loss = (loss.squeeze() * weights_batch).mean()

        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 10)
        self.optimizer.step()

        # Reset noise for exploration
        self.policy_net.reset_noise()
        self.target_net.reset_noise()

        return loss.item()

    def update_target_net(self):
        target_net_state_dict = self.target_net.state_dict()
        policy_net_state_dict = self.policy_net.state_dict()
        for key in policy_net_state_dict:
            target_net_state_dict[key] = policy_net_state_dict[key]*self.tau + policy_net_state_dict[key]*(1-self.tau)
        self.target_net.load_state_dict(target_net_state_dict)

# --- Training Loop ---
def train(num_episodes=25000):
    game = TicTacToe()
    agent1 = DQNAgent(player_letter=1)
    agent2 = DQNAgent(player_letter=-1)
    agents = {1: agent1, -1: agent2}

    # --- ENHANCED METRICS TRACKING ---
    history = {
        'loss': [], 'wins_x': [], 'wins_o': [], 'draws': [],
        'episode_lengths': [], 'avg_max_q_x': []
    }
    win_counts = {1: 0, -1: 0, 'draw': 0}

    # Define a fixed state to track Q-value evolution
    test_state = torch.tensor(np.zeros(9), dtype=torch.float32, device=agent1.device)

    beta_start = 0.4
    beta_frames = num_episodes
    beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)

    for i_episode in range(num_episodes):
        state = game.reset()
        current_player = 1
        done = False
        episode_transitions = []
        episode_length = 0

        while not done:
            episode_length += 1
            agent = agents[current_player]
            available_moves = game.available_moves()

            if not available_moves:
                done = True
                win_counts['draw'] += 1
                break

            action = agent.select_action(state, available_moves)
            episode_transitions.append({'state': state, 'action': action, 'player': current_player})
            game.make_move(action.item(), current_player)
            next_state = game.get_state()

            if game.current_winner is not None:
                done = True
                win_counts[game.current_winner] += 1

            state = next_state
            if not done:
                current_player = -current_player

        history['episode_lengths'].append(episode_length)

        # Assign rewards post-episode
        final_reward = {1: 0, -1: 0}
        if game.current_winner is not None:
            final_reward[game.current_winner] = 1
            final_reward[-game.current_winner] = -1

        for i, transition in enumerate(episode_transitions):
            player, s, a = transition['player'], transition['state'], transition['action']
            ns = None if i == len(episode_transitions) - 1 else episode_transitions[i+1]['state']
            r = final_reward[player] if done and i == len(episode_transitions) - 1 else 0
            agents[player].memory.push(s, a, ns, r)

        # Optimize and update
        beta = beta_by_frame(i_episode)
        loss1 = agents[1].optimize_model(beta)
        loss2 = agents[-1].optimize_model(beta)
        agents[1].update_target_net()
        agents[-1].update_target_net()

        avg_loss = np.mean([l for l in [loss1, loss2] if l is not None])
        if not np.isnan(avg_loss):
            history['loss'].append(avg_loss)

        if (i_episode + 1) % 100 == 0:
            history['wins_x'].append(win_counts[1])
            history['wins_o'].append(win_counts[-1])
            history['draws'].append(win_counts['draw'])
            win_counts = {1: 0, -1: 0, 'draw': 0}

            # Track Q-value for the test state
            with torch.no_grad():
                q_vals = agents[1].policy_net(test_state.unsqueeze(0))
                history['avg_max_q_x'].append(q_vals.max().item())

            print(f"Episode {i_episode+1}/{num_episodes}, Avg Loss: {np.mean(history['loss'][-100:]):.4f}")

    # --- ENHANCED PLOTTING DASHBOARD ---
    fig, axs = plt.subplots(2, 3, figsize=(20, 10))
    fig.suptitle('Comprehensive Agent Performance Dashboard', fontsize=20)

    # 1. Loss Plot
    loss_ma = np.convolve(history['loss'], np.ones(500)/500, mode='valid')
    axs[0, 0].plot(history['loss'], alpha=0.3, label='Per-Episode Loss')
    axs[0, 0].plot(loss_ma, color='orange', label='Moving Average (500 ep)')
    axs[0, 0].set_title('1. Training Loss', fontsize=14)
    axs[0, 0].set_xlabel('Episode')
    axs[0, 0].set_ylabel('Loss')
    axs[0, 0].legend()
    axs[0, 0].grid(True)

    # 2. Average Max Q-Value for Agent X
    axs[0, 1].plot(history['avg_max_q_x'], color='purple')
    axs[0, 1].set_title('2. Avg. Max Q-Value (Agent X, Start State)', fontsize=14)
    axs[0, 1].set_xlabel('Episode Chunk (x100)')
    axs[0, 1].set_ylabel('Q-Value')
    axs[0, 1].grid(True)

    # 3. Average Episode Length
    ep_len_ma = np.convolve(history['episode_lengths'], np.ones(500)/500, mode='valid')
    axs[0, 2].plot(history['episode_lengths'], alpha=0.3)
    axs[0, 2].plot(ep_len_ma, color='green', label='Moving Average (500 ep)')
    axs[0, 2].set_title('3. Episode Length', fontsize=14)
    axs[0, 2].set_xlabel('Episode')
    axs[0, 2].set_ylabel('Number of Moves')
    axs[0, 2].legend()
    axs[0, 2].grid(True)

    # 4. Game Outcome Proportions (Stacked Area)
    total_games_chunk = np.array(history['wins_x']) + np.array(history['wins_o']) + np.array(history['draws'])
    total_games_chunk[total_games_chunk == 0] = 1
    win_rate_x = np.array(history['wins_x']) / total_games_chunk
    win_rate_o = np.array(history['wins_o']) / total_games_chunk
    draw_rate = np.array(history['draws']) / total_games_chunk
    axs[1, 0].stackplot(range(len(win_rate_x)), win_rate_x, win_rate_o, draw_rate,
                        labels=['X Wins', 'O Wins', 'Draws'],
                        colors=['#1f77b4', '#d62728', '#2ca02c'])
    axs[1, 0].set_title('4. Game Outcome Proportions', fontsize=14)
    axs[1, 0].set_xlabel('Episode Chunk (x100)')
    axs[1, 0].set_ylabel('Proportion')
    axs[1, 0].legend(loc='upper right')
    axs[1, 0].grid(True)

    # 5. Win Rate (Line Plot)
    axs[1, 1].plot(win_rate_x, label='X Win Rate', color='#1f77b4', marker='.')
    axs[1, 1].plot(win_rate_o, label='O Win Rate', color='#d62728', marker='.')
    axs[1, 1].set_title('5. Win Rates per 100 Episodes', fontsize=14)
    axs[1, 1].set_xlabel('Episode Chunk (x100)')
    axs[1, 1].set_ylabel('Rate')
    axs[1, 1].legend()
    axs[1, 1].grid(True)

    # 6. Cumulative Game Outcomes
    cum_wins_x = np.cumsum(history['wins_x'])
    cum_wins_o = np.cumsum(history['wins_o'])
    cum_draws = np.cumsum(history['draws'])
    axs[1, 2].stackplot(range(len(cum_wins_x)), cum_wins_x, cum_wins_o, cum_draws,
                        labels=['X Wins', 'O Wins', 'Draws'],
                        colors=['#1f77b4', '#d62728', '#2ca02c'])
    axs[1, 2].set_title('6. Cumulative Game Outcomes', fontsize=14)
    axs[1, 2].set_xlabel('Episode Chunk (x100)')
    axs[1, 2].set_ylabel('Total Games')
    axs[1, 2].legend(loc='upper left')
    axs[1, 2].grid(True)

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

if __name__ == '__main__':
    train()